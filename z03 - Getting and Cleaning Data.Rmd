---
title: "R Notebook"
output: 
  html_notebook:
    toc: true
---


# Week 1 Quiz  
  
    
    

### q1  
```{r}
#data file
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"

download.file(fileUrl, destfile = "housingStateIdaho.csv"
              , method = "curl" )

#describe of variable names
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"

download.file(fileUrl, destfile = "housingStateIdaho_Describe.pdf"
              , method = "curl" )


#load variables
DT <- fread("housingStateIdaho.csv")

#summary
str(DT)
summary(DT)

# Q1 - Val >= 1000000
DT[,.N,VAL >= 24]

# Q2
summary(DT$FES)

```

### Q3

```{r}

install.packages("readxl", dependencies = TRUE)

library(readxl)


fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"

download.file(fileUrl, destfile = "natural-gas-acquisition-program.xlsx"
              , method = "curl" )

#load variables
DT <- read_xlsx("natural-gas-acquisition-program.xlsx", range = "A18:O23",sheet = 1, col_names = TRUE)

sum(DT$Zip*DT$Ext, na.rm=T)
```


### Q4  

```{r}

#install.packages('XML', dependencies = T)
library (XML)

fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
  
doc <- XML::xmlTreeParse(sub("s","",fileUrl), useInternalNodes = T)
rootNode <- xmlRoot(doc)  

#read a list of zipcodes 
# node put //
zipcodes <- xpathApply(rootNode,"//zipcode",xmlValue)

#convert in data Table
zipcodeDT <- data.table::data.table(zipcode = zipcodes)

#count
zipcodeDT[zipcode == "21231", .N]
```


### q5  
```{r}
library(data.table)

#data file
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"

download.file(fileUrl, destfile = "housingStateIdahoMicroSurvey.csv"
              , method = "curl" )


#load variables
DT <- fread("housingStateIdahoMicroSurvey.csv")

#summary
str(DT)
summary(DT)

DT[,mean(pwgtp15),by=SEX]





```





# Week 2 Quiz  


### q'  
```{r}

#copy from
#https://gist.github.com/mGalarnyk/ab0b1744fc718effa2759b2a1f17d60e
#mGalarnyk/GettingAndCleanDataWeek2Quiz.R

# 1. Register an application with the Github API here https://github.com/settings/applications. 
# Access the API to get information on your instructors repositories (hint: this is the url you want "https://api.github.com/users/jtleek/repos").
# Use this data to find the time that the datasharing repo was created. What time was it created?
# see https://medium.com/@GalarnykMichael/accessing-data-from-github-api-using-r-3633fb62cb08#.z0z07ph5h for more details.

#install.packages("jsonlite")
library(jsonlite)
#install.packages("httpuv")
library(httpuv)
#install.packages("httr")
library(httr)

# Can be github, linkedin etc depending on application
oauth_endpoints("github")

# Change based on your appname, key, and secret 
myapp <- oauth_app(appname = "Coursera_API",
                   key = "194da0704e36bd73eb2f",
                   secret = "a439461e1965d7b65bba1739cf27a841096115d7")

# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)

# Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)

# Take action on http error
stop_for_status(req)

# Extract content from a request
json1 = content(req)

# Convert to a data.frame
gitDF = jsonlite::fromJSON(jsonlite::toJSON(json1))

# Subset data.frame
gitDF[gitDF$full_name == "jtleek/datasharing", "created_at"] 

# 2. The sqldf package allows for execution of SQL commands on R data frames. We will use the sqldf package to practice the queries we might send with the dbSendQuery command in RMySQL.

# Download the American Community Survey data and load it into an R object called acs
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
# Which of the following commands will select only the data for the probability weights pwgtp1 with ages less than 50?

# install.packages("sqldf")
library("sqldf")

# need to use whem problem of .local in sqldf command
detach("package:RMySQL", unload=TRUE)

url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table::data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")

# 3. Using the same data frame you created in the previous problem, 
# what is the equivalent function to unique(acs$AGEP)

sqldf("select distinct AGEP from acs")

# 4. How many characters are in the 10th, 20th, 30th and 100th lines of HTML from this page:

#http://biostat.jhsph.edu/~jleek/contact.html

# (Hint: the nchar() function in R may be helpful)

 connection <- url("http://biostat.jhsph.edu/~jleek/contact.html")
 htmlCode <- readLines(connection)
 close(connection)
 c(nchar(htmlCode[10]), nchar(htmlCode[20]), nchar(htmlCode[30]), nchar(htmlCode[100]))


# 5. Read this data set into R and report the sum of the numbers in the fourth of the nine columns.

# https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for

# Original source of the data: http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for

# (Hint this is a fixed width file format)

url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
 lines <- readLines(url, n = 10)
 w <- c(1, 9, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3)
 colNames <- c("filler", "week", "filler", "sstNino12", "filler", "sstaNino12", 
              "filler", "sstNino3", "filler", "sstaNino3", "filler", "sstNino34", "filler", 
              "sstaNino34", "filler", "sstNino4", "filler", "sstaNino4")
d <- read.fwf(url, w, header = FALSE, skip = 4, col.names = colNames)
d <- d[, grep("^[^filler]", names(d))]
 sum(d[, 4])

```



# Dplyr


Practice

```{r}
fileURL <- "https://github.com/DataScienceSpecialization/courses/blob/master/03_GettingData/dplyr/chicago.rds?raw=true"

# 4. download data
download.file(fileURL, destfile = "chicago.rds", method = "curl", extra='-L')

chicago <- readRDS("chicago.rds")

#select
dim(chicago)

head(select(chicago, 1:5))

head(chicago)

names(chicago)[1:3]

head(select(chicago,city:dptp))

head(select(chicago, -(city:dptp)))

# filter
# dplyr copy data in another memory
chic.f <- filter(chicago,  pm25tmean2 > 30)

head(select(chic.f,  1:3, pm25tmean2, tmpd), 10)


#arrange
chicago <- arrange(chicago, date)
head(select(chicago, date, pm25tmean2), 10)
tail(select(chicago, date, pm25tmean2), 10)


chicago <- arrange(chicago, desc(date))
head(select(chicago, date, pm25tmean2), 10)
tail(select(chicago, date, pm25tmean2), 10)


#rename
head(chicago[, 1:5],5)
chicago <- rename(chicago, dewpoint = dptp,
                            pm25 = pm25tmean2)

head(chicago[, 1:5],5)

#mutate
chicago <- mutate(chicago, 
                  pm25detrend = pm25 - mean(pm25, 
                                            na.rm =T) )
head(select(chicago, pm25, pm25detrend))

#group_by (with factor)
chicago <- mutate(chicago, 
                  tempcat = factor(1*(tmpd>80),
                                   labels = c("cold",
                                              "hot")))
hotcold <- group_by(chicago, tempcat)
summarize(hotcold, pm25M = mean(pm25, na.rm=T),
            o3Max = max(o3tmean2),
            no2Mdn = median(no2tmean2))

#group_by (with factor)
chicago <- mutate(chicago, 
                  year = as.POSIXlt(date)$year + 1900)
years <- group_by(chicago, year)

summarize(years, pm25M = mean(pm25, na.rm=T),
            o3Max = max(o3tmean2, na.rm = T),
            no2Mdn = median(no2tmean2, na.rm = T))

#operator x %>% Y  like x next F(x,y)
chicago %>% mutate(month = as.POSIXlt(date)$mon + 1)  %>%
    group_by(month) %>%
     summarize(years, pm25M = mean(pm25, na.rm=T),
            o3Max = max(o3tmean2, na.rm = T),
            no2Mdn = median(no2tmean2, na.rm = T))



#teste indepence of data
head(chicago)
chicago$cityFactor <- factor(chicago$city)
head(chic.f)

#

```




# Merge data

```{r}
if(!file.exists("./data"))(dir.create("./data"))
# broke link
#fileUrl1 = "https://github.com/DataScienceSpecialization/courses/blob/master/03_GettingData/04_01_editingTextVariables/data/cameras.csv"
#fileUrl2 = "https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
#download.file(fileUrl1, destfile="./data/reviews.csv", method="curl")
#download.file(fileUrl2, destfile="./data/solutions.csv", method= 'curl')
reviews = read.csv("./data/reviews.csv"); solutions <- read.csv("./data/solutions.csv")
head(reviews,2)
head(solutions, 5)

```
### Merge

```{r}
head(reviews,2)

```

```{r}
head(solutions, 5)
```
```{r}
mergeData = merge(reviews, solutions, by.x="solution_id", by.y="id", all = T)
head(mergeData)
```

### join with plyr
```{r}
library(plyr);library(dplyr)
solutions$solution_id <- solutions[,'id']
full_join(reviews,solutions, by= 'solution_id')
arrange(join(reviews,solutions),id)
head(solutions)  
  
```



# Week 3 - Quiz

## Q1

### Read data and fast look into variables 
```{r read data}
library(tidyverse)


#data file
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"

download.file(fileUrl, destfile = "gss06hid.csv"
              , method = "curl" )

# read csv as tibble
gss06hid <- read_csv("gss06hid.csv")


#Data dict
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"

download.file(fileUrl, destfile = "gss06hidDataDict.pdf"
              , method = "curl" )

# Metodo mGarnik
agricultureLogical <- gss06hid$AGS >= 6 & gss06hid$ACR >=3
head(which(agricultureLogical),)



```

### Define The Goal, and initial path

**What I nedd?**  
Households with id(number of row)    
  area > 10 acres  
  sold > $10,000 agriculte products   
\    
\  
**info** from dataDic    
AGS 1  
Sales of Agriculture Products  
b .N/A (less than 1 acre/GQ/vacant/  
.2 or more units in structure)  
1 .None  
2 .$ 1 - $ 999  
3 .$ 1000 - $ 2499  
4 .$ 2500 - $ 4999  
5 .$ 5000 - $ 9999  
6 .$10000+  
\
\  
ACR 1  
Lot size  
b .N/A (GQ/not a one-family house or mobile home)  
1 .House on less than one acre  
2 .House on one to less than ten acres  
3 .House on ten or more acres  

## Q2

```{r}
# install.packages('jpeg')
library(jpeg)

# Download the file
download.file('https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg'
              , 'jeff.jpg'
              , mode='wb' )

# Read the image
picture <- jpeg::readJPEG('jeff.jpg'
                          , native=TRUE)

# Get Sample Quantiles corressponding to given prob
quantile(picture, probs = c(0.3, 0.8) )
```

### Q3

```{r}
# install.packages("data.table)
library("data.table")


# Download data and read FGDP data into data.table
FGDP <- data.table::fread('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
                          , skip=4
                          , nrows = 190
                          , select = c(1, 2, 4, 5)
                          , col.names=c("CountryCode", "Rank", "Economy", "Total")
                          )

# Download data and read FGDP data into data.table
FEDSTATS_Country <- data.table::fread('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv'
                                      )
                                      
mergedDT <- merge(FGDP, FEDSTATS_Country, by = 'CountryCode')

# How many of the IDs match?
nrow(mergedDT)

# Answer: 
# 189

# Sort the data frame in descending order by GDP rank (so United States is last). 
# What is the 13th country in the resulting data frame?
mergedDT[order(-Rank)][12,.(Economy)]

# Answer: 

#                Economy
# 1: St. Kitts and Nevis
```
### Q4

```{r}
# "High income: OECD" 
mergedDT[`Income Group` == "High income: OECD"
         , lapply(.SD, mean)
         , .SDcols = c("Rank")
         , by = "Income Group"]

# Answer:
#
#         Income Group     Rank
# 1: High income: OECD 32.96667

# "High income: nonOECD"
mergedDT[`Income Group` == "High income: nonOECD"
         , lapply(.SD, mean)
         , .SDcols = c("Rank")
         , by = "Income Group"]

# Answer
#            Income Group     Rank
# 1: High income: nonOECD 91.91304
```

### Q5
Cut the GDP ranking into 5 separate quantile groups. Make a table versus Income.Group. How many countries are Lower middle income but among the 38 nations with highest GDP?
```{r}
# install.packages('dplyr')
library('dplyr')

breaks <- quantile(mergedDT[, Rank], probs = seq(0, 1, 0.2), na.rm = TRUE)
mergedDT$quantileGDP <- cut(mergedDT[, Rank], breaks = breaks)
mergedDT[`Income Group` == "Lower middle income", .N, by = c("Income Group", "quantileGDP")]

# Answer 
#           Income Group quantileGDP  N
# 1: Lower middle income (38.6,76.2] 13
# 2: Lower middle income   (114,152]  9
# 3: Lower middle income   (152,190] 16
# 4: Lower middle income  (76.2,114] 11
# 5: Lower middle income    (1,38.6]  5
```



<!--   end topic    -->
<a href="#top">
  Back to top of page
  </a> 
<br> <br> <br> <br> <br> 

  
  
  
  
<!-- ###############   New   Topic    ##################   --> 

# Week 4

### video 1

```{r}
(!file.exists("./data"))(dir.create("./data"))
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv", method="curl" )
cameraData <- read.csv("./data/cameras.csv")

names (cameraData)

tolower(names(cameraData))
```
Fixing character vectors 
```{r}

```
\



### quiz 4

# Getting and Cleaning Data Quiz 4 (JHU) Coursera

Question 1
----------
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv

and load the data into R. The code book, describing the variable names is here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf

Apply strsplit() to split all the names of the data frame on the characters "wgtp". What is the value of the 123 element of the resulting list?


```{r}
library("data.table")
communities <- data.table::fread("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
varNamesSplit <- strsplit(names(communities), "wgtp")
varNamesSplit[[123]]

# Answer 
# ""   "15"
```


Question 2
----------
Load the Gross Domestic Product data for the 190 ranked countries in this data set:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv

Remove the commas from the GDP numbers in millions of dollars and average them. What is the average?

Original data sources: http://data.worldbank.org/data-catalog/GDP-ranking-table

```{r}
# Removed the s from https to be compatible with windows computers. 
# Skip first 5 rows and only read in relevent columns
GDPrank <- data.table::fread('http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
                    , skip=5
                    , nrows=190
                    , select = c(1, 2, 4, 5)
                    , col.names=c("CountryCode", "Rank", "Country", "GDP")
)

# Remove the commas using gsub
# Convert to integer after removing commas. 
# Take mean of GDP column (I know this code may look a little confusing)
GDPrank[, mean(as.integer(gsub(pattern = ',', replacement = '', x = GDP )))]
GDPrank[, mean(as.integer(gsub(pattern = ',', replacement = '', x = GDP)))]
GDPrank[,.(GDP)]

# Answer: 
# 377652.4

```


Question 3
----------
In the data set from Question 2 what is a regular expression that would allow you to count the number of countries whose name begins with "United"? Assume that the variable with the country names in it is named countryNames. How many countries begin with United?

```{r}
# Answer: 
grep("^United",GDPrank[, Country])

# how is country
GDPrank[grep("^United",GDPrank[, Country]), Country]

```


Question 4
----------
Load the Gross Domestic Product data for the 190 ranked countries in this data set:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv

Load the educational data from this data set:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv

Match the data based on the country shortcode. Of the countries for which the end of the fiscal year is available, how many end in June?

Original data sources: http://data.worldbank.org/data-catalog/GDP-ranking-table http://data.worldbank.org/data-catalog/ed-stats
````{r}
GDPrank <- data.table::fread('http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
                             , skip=5
                             , nrows=190
                             , select = c(1, 2, 4, 5)
                             , col.names=c("CountryCode", "Rank", "Country", "GDP")
)

eduDT <- data.table::fread('http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv')

mergedDT <- merge(GDPrank, eduDT, by = 'CountryCode')

mergedDT[grepl(pattern = "Fiscal year end: June 30;", mergedDT[, `Special Notes`]), .N]

#denis
countrys <- merge(GDPrank, eduDT, by = 'CountryCode')

countrys[grepl(pattern = "Fiscal year end: June 30;", countrys[, `Special Notes`]), .N ]

names(countrys)

# Answer: 
# 13
```

Question 5
----------
You can use the quantmod (http://www.quantmod.com/) package to get historical stock prices for publicly traded companies on the NASDAQ and NYSE. Use the following code to download data on Amazon's stock price and get the times the data was sampled.
```{R}
library(quantmod) 
amzn = getSymbols("AMZN",auto.assign=FALSE) 
sampleTimes = index(amzn)
```
How many values were collected in 2012? How many values were collected on Mondays in 2012?
```{R}
# install.packages("quantmod")
library("quantmod")
amzn <- getSymbols("AMZN",auto.assign=FALSE)
sampleTimes <- index(amzn) 
timeDT <- data.table::data.table(timeCol = sampleTimes)

# How many values were collected in 2012? 
timeDT[(timeCol >= "2012-01-01") & (timeCol) < "2013-01-01", .N ]
# Answer: 
# 250

# How many values were collected on Mondays in 2012?
timeDT[((timeCol >= "2012-01-01") & (timeCol < "2013-01-01")) & (weekdays(timeCol) == "Monday"), .N ]
# Answer:
# 47
```



### Load price of stock
You can use the quantmod (http://www.quantmod.com/) package to get historical stock prices for publicly traded companies on the NASDAQ and NYSE. Use the following code to download data on Amazon's stock price and get the times the data was sampled.
```{R}
#install.packages("quantmod")
library(quantmod) 
amzn = getSymbols("AMZN",auto.assign=FALSE) 
sampleTimes = index(amzn)
```
How many values were collected in 2012? How many values were collected on Mondays in 2012?
```{R}
# install.packages("quantmod")
library("quantmod")
amzn <- getSymbols("AMZN",auto.assign=FALSE)
sampleTimes <- index(amzn) 
timeDT <- data.table::data.table(timeCol = sampleTimes)

# How many values were collected in 2012? 
timeDT[(timeCol >= "2012-01-01") & (timeCol) < "2013-01-01", .N ]
# Answer: 
# 250

# How many values were collected on Mondays in 2012?
timeDT[((timeCol >= "2012-01-01") & (timeCol < "2013-01-01")) & (weekdays(timeCol) == "Monday"), .N ]

#in brazil monday -> segunda-feira
timeDT[((timeCol >= "2012-01-01") & (timeCol < "2013-01-01")) & (weekdays(timeCol) == "segunda-feira"), .N ]

# Answer:
# 47

#denis
timeDT[, weekdays(timeCol)]
```

<a name="top"></a>  <br> <br>    




<!-- ###############   New   Topic    ##################   --> 

# Course Project




### temp  
```{r}
download.file('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
              , destfile = "getdata_data_GDP.csv"
              , method = "curl" )


```


